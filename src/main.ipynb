{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e1730f5",
   "metadata": {},
   "source": [
    "# Fine Tuning for Summarisation Task\n",
    "## Introduction\n",
    "As stated, trying to perform the task of abstractive summarisation through fine tuning a T5 model. As the T5 model has both encoder and the decoder pre-trained, fine-tuning it on dataset should be great start for the task.\n",
    "\n",
    "While T5 is pre-trained for summarisation task on normal CNN/Daily Mail dataset already, this serves as a demonstration to show how to do it for any domain specific summarisation if needed. \n",
    "\n",
    "We are also fine-tuning using Low-Rank Adaptation (LoRA), therefore only small number of parameters have to be fine-tuned for the task, that will augment the baseline model. We will compare then compare its performance in summarisation against the standard non fine-tuned instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e3364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import section\n",
    "from bert_score import score\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.training_args import TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from typing import cast\n",
    "\n",
    "from utils import preprocess_function, get_model_name, get_tokenizer, get_data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d3edcc",
   "metadata": {},
   "source": [
    "## Section 1: Preparing the Dataset\n",
    "The CNN/Daily Mail Dataset of News Articles and their highlights have been hosted as a [huggingface dataset](https://huggingface.co/datasets/abisee/cnn_dailymail) and therefore can be downloaded through the `datasets` library of huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe13983",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "dataset = cast(DatasetDict, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230acacb",
   "metadata": {},
   "source": [
    "### 1.1. Inspect the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477dbd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset['train'][0]\n",
    "\n",
    "print(\"Article:\")\n",
    "print(sample['article'][:300])\n",
    "print(\"\")\n",
    "print(\"Summary:\")\n",
    "print(sample['highlights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of samples present\n",
    "print(f\"Number of training samples: {len(dataset['train'])}\")\n",
    "print(f\"Number of validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"Number of test samples: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1fdee4",
   "metadata": {},
   "source": [
    "### 1.2. Split dataset into tokens ready for consumption by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7255f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4878b029",
   "metadata": {},
   "source": [
    "## Section 2: Creating the model and the LoRA Config\n",
    "The hugging face interface makes it very easy to perform fine-tuning using LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a4a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional Generation is needed over raw hidden encoder decoder stats from T5Model for this task.\n",
    "# This comes with the needed vocabulary logits for generating the summary tokens.\n",
    "model = T5ForConditionalGeneration.from_pretrained(get_model_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17583f5",
   "metadata": {},
   "source": [
    "We create the following LoRA config\n",
    "1. Use rank 8 to reduce the number of parameters.\n",
    "2. Alpha influences how much LoRA matrix contributes to the final output.\n",
    "3. We target the Query and Values part of the attention module in the model for Adaptation, as they are the most impactful.\n",
    "4. Adding dropout of 0.05 for better regularisation.\n",
    "5. Biases are not adapted as of now.\n",
    "6. Since it generates a summary from article, it is a sequence to sequence task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b22d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Config\n",
    "lora_config = LoraConfig(r=8, lora_alpha=16, target_modules=[\"q\", \"v\"],\n",
    "                         lora_dropout=0.05, bias=\"none\", task_type=TaskType.SEQ_2_SEQ_LM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3dd920",
   "metadata": {},
   "source": [
    "We then add LoRA adapter to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Show how many parameters we train for indicating efficiency.\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d71957f",
   "metadata": {},
   "source": [
    "## Section 3: Creating a Trainer\n",
    "Now that we have obtained the appropriate tokens for the model to consume from the dataset and created a LoRA wrapped model instance for fine-tuning, we will create the trainer instance to actually train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5581af14",
   "metadata": {},
   "source": [
    "Creating an instance of the TrainingArguments to be supplied to the Trainer.\n",
    "1. Saving the weights to the results folder.\n",
    "2. Evaluating the performance every 500 steps and logging progress every 100 steps.\n",
    "3. On a training and evaluation batch size of 16.\n",
    "4. With a very small learning rate of 1e-5 as it is a fine tuning task.\n",
    "5. Warm up starts with a lower learning rate and then gradually increases to our set learning rate to ensure stability.\n",
    "6. Save the weights every 1000 steps and only retain the 2 most recent checkpoints.\n",
    "7. Use mixed precision for faster training.\n",
    "8. Save the logs to the logs folder and no remote report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f4330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust these according to your hardware constraints and performance requirements.\n",
    "TOTAL_EPOCHS=5\n",
    "TRAIN_BATCH=16\n",
    "EVAL_BATCH=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f7fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TrainingArguments instance to give the trainer its configuration.\n",
    "training_args = TrainingArguments(output_dir='./results', eval_steps=500, logging_steps=100, \n",
    "                                  per_device_train_batch_size=TRAIN_BATCH, per_device_eval_batch_size=EVAL_BATCH, \n",
    "                                  num_train_epochs=TOTAL_EPOCHS, learning_rate=1e-5, \n",
    "                                  warmup_steps=200, save_steps=1000, save_total_limit=2, fp16=True,\n",
    "                                  logging_dir='./logs', report_to='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba2cc4d",
   "metadata": {},
   "source": [
    "### 3.1. Create an instance of Trainer for training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e358264",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=training_args, train_dataset=dataset['train'], \n",
    "                  eval_dataset=dataset['validation'], data_collator=get_data_collator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97b5e3e",
   "metadata": {},
   "source": [
    "### 3.2. Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae1e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555a90b",
   "metadata": {},
   "source": [
    "### 3.3. Save the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0780935",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"t5-small-lora-ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ec79a9",
   "metadata": {},
   "source": [
    "## Section 4: Generate a summary from a real article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a5c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch our fine-tuned model to eval mode, to prevent calculation of gradients.\n",
    "model.eval()\n",
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d572933",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_article = \"SHORT PARAGRAPH HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e0d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = \"summarize: \" + real_article\n",
    "inputs = tokenizer(article_text, return_tensors=\"pt\",\n",
    "                   truncation=True, max_length=512)\n",
    "\n",
    "# Generate summary\n",
    "outputs = model.generate(**inputs, max_length=128, num_beams=4)\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7d46ef",
   "metadata": {},
   "source": [
    "## Section 5: Compare with Baseline on summarisation performance\n",
    "BERTScore will be used as it compares the semantic meaning over literal n-gram overlap (as in the case of ROUGE) and therefore is better suited to measure performance of an abstractive summarisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdad029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model instance to compare performance against\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "test_set = dataset['test']\n",
    "test_set = cast(Dataset, test_set)\n",
    "baseline_summaries = []\n",
    "finetuned_summaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a0ac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the summaries from both model instances\n",
    "for item in test_set:\n",
    "    input_text = \"summarize: \" + item[\"article\"]\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\",\n",
    "                       truncation=True, max_length=512)\n",
    "    output1 = base_model.generate(**inputs, max_length=128)\n",
    "    output2 = model.generate(**inputs, max_length=128)\n",
    "    summary1 = tokenizer.decode(output1[0], skip_special_tokens=True)\n",
    "    summary2 = tokenizer.decode(output2[0], skip_special_tokens=True)\n",
    "    baseline_summaries.append(summary1)\n",
    "    finetuned_summaries.append(summary2)\n",
    "\n",
    "# Compute the score\n",
    "references = [item[\"highlights\"] for item in test_set]\n",
    "P_base = score(baseline_summaries, references, lang=\"en\")\n",
    "P_finetuned = score(finetuned_summaries, references, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe38ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Base T5 BERTScore F1: {P_base[2].mean().item():.4f}\")\n",
    "print(f\"LoRA-Tuned T5 BERTScore F1: {P_finetuned[2].mean().item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
